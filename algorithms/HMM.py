from torch.optim import Adam
from torch import nn, zeros, cat
from torch.nn.functional import one_hot
from networks.DiagonalGaussian import DiagonalGaussian as Gaussian


#
# Class implementing a Hidden Markov Model.
#
class HMM:

    def __init__(self, vae, n_states, n_actions):
        """
        Constructor.
        :param vae: the Variational Auto-Encoder.
        """

        # Store the Variational Auto-Encoder
        self.__vae = vae

        # Prior over hidden states
        self.__mean_p = zeros(n_states, requires_grad=False)
        self.__log_variance_p = zeros(n_states, requires_grad=False)

        # Create the default transition model
        self.__transition_net = nn.Sequential(
            nn.Linear(n_states + n_actions, 10),
            nn.ReLU(),
            Gaussian(10, n_states)
        )

        # Create optimizer of the transition model.
        self.__optimizer = Adam(self.__transition_net.parameters(), lr=0.0001)

        # Stores number of states and actions
        self.__n_states = n_states
        self.__n_actions = n_actions

    def training_step(self, buffer, batch_size):
        """
        Perform one training iteration of the HMM.
        :param buffer: the replay buffer.
        :param batch_size: the size of the batch to sample.
        :return: a triple (loss_vae, loss_transition, images) where
        - loss_vae: is the loss of the Variational Auto-Encoder
        - loss_transition: is the loss of the transition model
        - images: the images generated by the VAE.
        """

        # Sample batch from replay buffer.
        obs, actions, _, _, next_obs = buffer.sample(batch_size)

        # Perform inference of hidden state for the observation and next observation.
        loss_vae, image, means_q1, log_variances_q1 = \
            self.__vae.training_step(obs, self.__mean_p, self.__log_variance_p, 0.001)
        _,        _,    means_q2, log_variances_q2 = \
            self.__vae.training_step(obs, self.__mean_p, self.__log_variance_p, 0.001)

        # Create the input of the transition model.
        z = self.__vae.sample_z(means_q1.detach(), log_variances_q1.detach(), image.shape[0])
        actions = one_hot(actions, self.__n_actions)
        inputs = cat((z, actions), dim=1).detach()

        # Predict the means and variances of the next observations.
        means, log_variances = self.__transition_net(inputs)

        # Create the loss function
        loss_means = nn.MSELoss()(means, means_q2.detach())
        loss_log_variances = nn.MSELoss()(log_variances, log_variances_q2.detach())
        loss = loss_means + loss_log_variances

        # Perform one step of gradient descent
        self.__optimizer.zero_grad()
        loss.backward()
        self.__optimizer.step()

        return loss_vae, loss, image
